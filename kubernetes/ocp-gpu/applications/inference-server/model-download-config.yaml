apiVersion: v1
kind: ConfigMap
metadata:
  name: model-download-config
data:
  # HuggingFace repository (required)
  MODEL_REPO: "RedHatAI/Qwen3-30B-A3B-quantized.w4a16"

  # Specific file to download from repo (optional)
  # If empty, downloads entire repo and uses repo name as model file
  # If set, downloads only this file
  MODEL_FILE: ""

  # Generic served model name for API
  SERVED_MODEL_NAME: "local-llm"

  # vLLM memory settings
  # Fraction of GPU VRAM to use (auto-reduces by 1% on OOM, higher = more context)
  GPU_MEMORY_UTILIZATION: "0.94"
  # Starting value for auto-detection (set high to trigger error and discover actual limit)
  MAX_MODEL_LEN_START: "999999"

  # Examples:
  # 1. Download entire repo:
  #    MODEL_REPO: "RedHatAI/granite-3.1-8b-instruct-quantized.w4a16"
  #    MODEL_FILE: ""
  #    Result: Downloads whole repo, vLLM uses "granite-3.1-8b-instruct-quantized.w4a16"
  #
  # 2. Download specific file from repo:
  #    MODEL_REPO: "unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF"
  #    MODEL_FILE: "Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf"
  #    Result: Downloads specific file, vLLM uses "Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf"
