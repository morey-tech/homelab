kind: Secret
apiVersion: v1
metadata:
  name: inference-server-config
  annotations:
    openshift.io/description: Connect to a LLM model
    openshift.io/display-name: Self-Hosted LLM on OCP
stringData:
  DISABLE_TELEMETRY: "true"
  GENERIC_OPEN_AI_API_KEY: ""
  GENERIC_OPEN_AI_BASE_PATH: "http://inference-server.inference-server.svc.cluster.local:80/v1"
  GENERIC_OPEN_AI_MAX_TOKENS: "4096"
  GENERIC_OPEN_AI_MODEL_PREF: "Qwen3-Coder-30B-A3B-Instruct-Q4_K_M"
  LLM_PROVIDER: "generic-openai"
  GENERIC_OPEN_AI_MODEL_TOKEN_LIMIT: "8192"
  VECTOR_DB: "lancedb"
  EMBEDDING_ENGINE: "native"
type: Opaque