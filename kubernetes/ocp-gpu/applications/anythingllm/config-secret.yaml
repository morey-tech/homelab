kind: Secret
apiVersion: v1
metadata:
  name: inference-server-config
  annotations:
    openshift.io/description: Connect to a LLM model
    openshift.io/display-name: Self-Hosted LLM on OCP
stringData:
  DISABLE_TELEMETRY: "true"
  GENERIC_OPEN_AI_API_KEY: ""
  GENERIC_OPEN_AI_BASE_PATH: "http://vllm.vllm-cpu.svc.cluster.local:8000/v1"
  GENERIC_OPEN_AI_MAX_TOKENS: "4096"
  GENERIC_OPEN_AI_MODEL_PREF: "local-llm"
  LLM_PROVIDER: "generic-openai"
  GENERIC_OPEN_AI_MODEL_TOKEN_LIMIT: "32768"
  VECTOR_DB: "lancedb"
  EMBEDDING_ENGINE: "native"
type: Opaque