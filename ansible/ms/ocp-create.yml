---
- name: Set Up cluster
  hosts: ocp-clusters
  gather_facts: false  # Hosts aren't available, since this playbook creates them.

  vars:
    create_assisted_installer: true
    recreate: false

  tasks:
    - name: Assisted Installer
      block:
        - name: Get a list of clusters  # noqa: run-once[task]
          rhpds.assisted_installer.list_clusters:
            offline_token: "{{ rh_console_offline_token }}"
          register: listclusters
          run_once: true

        - name: Delete Existing Assisted Installers (When Recreate)
          rhpds.assisted_installer.delete_cluster:
            cluster_id: "{{ item.id }}"
            offline_token: "{{ rh_console_offline_token }}"
          loop: "{{ listclusters.result | selectattr('name', 'equalto', cluster_name) | list }}"
          loop_control:
            label: "{{ item.name }}, {{ item.id }}"
          when: recreate

        - name: Set cluster_id to existing installer
          ansible.builtin.set_fact:
            # Last should be the most recent one.
            cluster_id: "{{ listclusters.result | selectattr('name', 'equalto', cluster_name) | map(attribute='id') | last }}"
          # loop: "{{ listclusters.result | selectattr('name', 'equalto', cluster_name) | list }}"
          # loop_control:
          #   label: "{{ item.name }}, {{ item.id }}"
          # register: cluster_exists
          when: listclusters.result | selectattr('name', 'equalto', cluster_name) | list | length > 0

        - name: Create Assisted Installer Cluster
          rhpds.assisted_installer.create_cluster:
            name: "{{ cluster_name }}"
            openshift_version: "{{ cluster_version }}"
            base_dns_domain: "{{ cluster_dns_zone }}"
            offline_token: "{{ rh_console_offline_token }}"
            pull_secret: "{{ rh_console_pull_secret | to_json }}"
            # If node group is less than 3, set high availability mode to None for SNO.
            high_availability_mode: "{{ 'Full' if groups[nodes_group] | length >= 3 else 'None' }}"
            cluster_network_cidr: "{{ cluster_network_cidr }}"
            cluster_network_host_prefix: "{{ cluster_network_host_prefix }}"
            service_networks: "{{ service_networks }}"
            machine_networks: "{{ machine_networks }}"
            api_vips: "{{ api_vips if groups[nodes_group] | length >= 3 else [] }}"
            ingress_vips: "{{ ingress_vips if groups[nodes_group] | length >= 3 else [] }}"
            ssh_public_key: "{{ ssh_authorized_keys }}"
          register: newcluster
          when: not cluster_id or recreate

        - name: Set cluster_id to new installer
          ansible.builtin.set_fact:
            cluster_id: "{{ newcluster.result.id }}"
          when: not cluster_id or recreate

        # debug cluster_id
        - name: Print selected cluster_id
          ansible.builtin.debug:
            msg: "{{ cluster_id }}"

        - name: Print link to assisted installer # noqa: run-once[task]
          ansible.builtin.debug:
            msg: "https://console.redhat.com/openshift/assisted-installer/clusters/{{ cluster_id }}"
          when: create_assisted_installer

    # break point
    - name: Fail
      ansible.builtin.fail:
        msg: "Break"

    - name: Create DHCP static mapping
      pfsensible.core.pfsense_dhcp_static:
        netif: opt3  # rh_lab
        hostname: "{{ hostvars[item].inventory_hostname_short }}"
        macaddr: "{{ hostvars[item].mac_address }}"
        ipaddr: "{{ hostvars[item].ip_address }}"
        ddnsdomainkeyalgorithm: "hmac-md5"  # fixes diff.
        # nameserver: "{{ hostvars['dns.rh-lab.morey.tech'].ip_address }}"
        state: present
      delegate_to: pfsense.home.morey.tech
      throttle: 1
      loop: "{{ groups[nodes_group] }}"

    - name: Add DNS zone to named.conf.local
      ansible.builtin.blockinfile:
        path: /etc/bind/named.conf.local
        block: |
          zone "{{ cluster_name }}.{{ cluster_dns_zone }}" {
              type master;
              file "/etc/bind/db.{{ cluster_name }}.{{ cluster_dns_zone }}";
          };
        marker: "// {mark} ANSIBLE MANAGED BLOCK for {{ cluster_name }}.{{ cluster_dns_zone }}"
        insertafter: "EOF"
        append_newline: true
        prepend_newline: true
        state: present
      notify: Reload BIND9
      delegate_to: dns.rh-lab.morey.tech

    - name: Initialize node_dns_records
      ansible.builtin.set_fact:
        node_dns_records: []
      delegate_to: dns.rh-lab.morey.tech

    - name: Add each node DNS record to variable
      ansible.builtin.set_fact:
        node_dns_records: "{{ node_dns_records + [{'name': item.split('.')[0], 'ip': hostvars[item].ip_address}] }}"
      loop: "{{ groups[nodes_group] }}"
      delegate_to: dns.rh-lab.morey.tech

    - name: Configure DNS zone
      ansible.builtin.template:
        src: db.j2
        dest: "/etc/bind/db.{{ cluster_name }}.{{ cluster_dns_zone }}"
        mode: '0644'
      notify: Reload BIND9
      vars:
        dns_zone: "{{ cluster_name }}.{{ cluster_dns_zone }}"
        # SNO clusters don't have a VIP. Therefore, use the first node's IP.
        dns_records: "{{ node_dns_records + [
          { 'name': 'api', 'ip': api_vips.0.ip | default(hostvars[groups[nodes_group][0]].ip_address) },
          { 'name': 'api-int', 'ip': api_vips.0.ip | default(hostvars[groups[nodes_group][0]].ip_address) },
          { 'name': '*.apps', 'ip': ingress_vips.0.ip | default(hostvars[groups[nodes_group][0]].ip_address) }] }}"
        dns_ip_address: "{{ hostvars['dns.rh-lab.morey.tech'].ip_address }}"
      delegate_to: dns.rh-lab.morey.tech

    - name: Run Reload BIND9 handler
      ansible.builtin.meta: flush_handlers
      delegate_to: dns.rh-lab.morey.tech

    - name: Create new PVE pool
      community.general.proxmox_pool:
        api_user: "{{ pve_api_user }}"
        api_token_id: "{{ pve_api_token_id }}"
        api_token_secret: "{{ pve_api_token_secret }}"
        api_host: "{{ pve_api_host }}"
        poolid: "{{ cluster_name }}"
        comment: "{{ cluster_name }} cluster"

    - name: Create VMs
      community.general.proxmox_kvm:
        api_user: "{{ pve_api_user }}"
        api_token_id: "{{ pve_api_token_id }}"
        api_token_secret: "{{ pve_api_token_secret }}"
        api_host: "{{ pve_api_host }}"
        vmid: "{{ hostvars[item].pve_vm_id }}"
        name: "{{ hostvars[item].inventory_hostname_short }}"
        node: "{{ hostvars[item].pve_node }}"
        pool: "{{ cluster_name }}"
        cpu: x86-64-v2-AES
        cores: "{{ hostvars[item].cores }}"
        memory: "{{ hostvars[item].memory }}"
        scsihw: virtio-scsi-single
        scsi:
          scsi0: 'local-nvme:128,iothread=1'
        net:
          net0: 'virtio={{ hostvars[item].mac_address }},bridge=vmbr0,tag={{ hostvars[item].vlan_tag }},firewall=1'
        # update: true  # Doesn't work if VM already exists.
      loop: "{{ groups[nodes_group] }}"
      register: create_vms
      # "msg": "VM ocp-lab-01 with vmid 6101 updated",  == already exists, maybe updated
      # "msg": "VM ocp-lab-01 with vmid 6101 deployed", == create

    - name: Check if Discovery ISO exists
      ansible.builtin.stat:
        path: "/mnt/pve/storage-pve/template/iso/{{ cluster_name }}-{{ cluster_id }}-discovery.iso"
      delegate_to: "{{ groups['pvems'][0] }}"  # Pick a host from the hypervisor group
      register: discovery_iso_check
      when: create_assisted_installer and create_vms.changed

    # - name: debug discovery_iso_check
    #   ansible.builtin.debug:
    #     var: discovery_iso_check

    - name: Create Discovery ISO # noqa: run-once[task]
      rhpds.assisted_installer.create_infra_env:
        name: "{{ cluster_name }}-infra-env"
        image_type: "{{ image_type }}"
        cluster_id: "{{ cluster_id }}"
        ssh_authorized_key: "{{ ssh_authorized_keys }}"
        offline_token: "{{ rh_console_offline_token }}"
        pull_secret: "{{ rh_console_pull_secret | to_json }}"
        openshift_version: "{{ cluster_version }}"
      register: newinfraenv
      when: recreate or (not discovery_iso_check.stat.exists | default(true))

    - name: Download Discovery ISO
      ansible.builtin.get_url:
        url: "{{ newinfraenv.result.download_url }}"
        dest: "/mnt/pve/storage-pve/template/iso/{{ cluster_name }}-{{ cluster_id }}-discovery.iso"
        mode: '0777'
        force: true
      delegate_to: "{{ groups['pvems'][0] }}"  # Pick a host from the hypervisor group
      when: newinfraenv.changed or (not discovery_iso_check.stat.exists | default(true))
      register: download_discovery_iso
      # https://docs.ansible.com/ansible/latest/playbook_guide/playbooks_async.html#run-tasks-concurrently-poll-0

    - name: Add IDE2 to VMs
      community.general.proxmox_kvm:
        api_user: "{{ pve_api_user }}"
        api_token_id: "{{ pve_api_token_id }}"
        api_token_secret: "{{ pve_api_token_secret }}"
        api_host: "{{ pve_api_host }}"
        vmid: "{{ hostvars[item].pve_vm_id }}"
        node: "{{ hostvars[item].pve_node }}"
        ide:
          ide2: "storage-pve:iso/{{ cluster_name }}-{{ cluster_id }}-discovery.iso,media=cdrom"
        # boot: order=scsi0;ide2;net0;
        update: true
        update_unsafe: true
      loop: "{{ groups[nodes_group] }}"
      when: download_discovery_iso.changed

    - name: Add boot order to VMs
      community.general.proxmox_kvm:
        api_user: "{{ pve_api_user }}"
        api_token_id: "{{ pve_api_token_id }}"
        api_token_secret: "{{ pve_api_token_secret }}"
        api_host: "{{ pve_api_host }}"
        vmid: "{{ hostvars[item].pve_vm_id }}"
        node: "{{ hostvars[item].pve_node }}"
        # ide:
        #   ide2: "storage-pve:iso/{{ cluster_name }}-{{ cluster_id }}-discovery.iso,media=cdrom"
        boot: order=scsi0;ide2;net0;
        update: true
      loop: "{{ groups[nodes_group] }}"
      when: download_discovery_iso.changed

    # - name: Debug create_vms
    #   ansible.builtin.debug:
    #     var: create_vms

    # Stop with force first before destroy to speed up.
    - name: Stop Existing VMs
      community.general.proxmox_kvm:
        api_user: "{{ pve_api_user }}"
        api_token_id: "{{ pve_api_token_id }}"
        api_token_secret: "{{ pve_api_token_secret }}"
        api_host: "{{ pve_api_host }}"
        vmid: "{{ hostvars[item].pve_vm_id }}"
        name: "{{ hostvars[item].inventory_hostname_short }}"
        node: "{{ hostvars[item].pve_node }}"
        state: stopped
        force: true
        timeout: 15
      loop: "{{ groups[nodes_group] }}"
      when: download_discovery_iso.changed

    - name: Recreate boot disk
      community.general.proxmox_disk:
        api_user: "{{ pve_api_user }}"
        api_token_id: "{{ pve_api_token_id }}"
        api_token_secret: "{{ pve_api_token_secret }}"
        api_host: "{{ pve_api_host }}"
        vmid: "{{ hostvars[item].pve_vm_id }}"
        name: "{{ hostvars[item].inventory_hostname_short }}"
        # scsi:
        #   scsi0: 'local-nvme:128,iothread=1'
        disk: scsi0
        format: raw
        storage: local-nvme
        size: 128
        create: forced  # recreate
        iothread: 1
        state: present
      loop: "{{ groups[nodes_group] }}"
      when: download_discovery_iso.changed

    - name: Pause before starting VMs
      ansible.builtin.wait_for:
        timeout: 3

    - name: Start VMs
      community.general.proxmox_kvm:
        api_user: "{{ pve_api_user }}"
        api_token_id: "{{ pve_api_token_id }}"
        api_token_secret: "{{ pve_api_token_secret }}"
        api_host: "{{ pve_api_host }}"
        vmid: "{{ hostvars[item].pve_vm_id }}"
        name: "{{ hostvars[item].inventory_hostname_short }}"
        node: "{{ hostvars[item].pve_node }}"
        state: started
      loop: "{{ groups[nodes_group] }}"

    # - name: Fail
    #   ansible.builtin.fail:
    #     msg: "Break"

    - name: Print link to assisted installer # noqa: run-once[task]
      ansible.builtin.debug:
        msg: "https://console.redhat.com/openshift/assisted-installer/clusters/{{ cluster_id }}"
      when: create_assisted_installer

    - name: Wait for the hosts to be ready
      rhpds.assisted_installer.wait_for_hosts:
        cluster_id: "{{ cluster_id }}"
        offline_token: "{{ rh_console_offline_token }}"
        expected_hosts: "{{ groups[nodes_group] | length }}"
        wait_timeout: 300  # Usually ready in 3 - 4 minutes with "full" discovery ISO.
      when: newinfraenv.changed or create_vms.changed
      register: wait_for_hosts
      until: wait_for_hosts.result.status == "ready"
      retries: 3  # On the off chance that the request fails.

    - name: Remove discovery ISO from boot order on VMs
      community.general.proxmox_kvm:
        api_user: "{{ pve_api_user }}"
        api_token_id: "{{ pve_api_token_id }}"
        api_token_secret: "{{ pve_api_token_secret }}"
        api_host: "{{ pve_api_host }}"
        vmid: "{{ hostvars[item].pve_vm_id }}"
        node: "{{ hostvars[item].pve_node }}"
        boot: order=scsi0;net0;
        update: true
      loop: "{{ groups[nodes_group] }}"
      # when: newinfraenv.changed

    - name: Start cluster installation
      rhpds.assisted_installer.install_cluster:
        cluster_id: "{{ cluster_id }}"
        offline_token: "{{ rh_console_offline_token }}"
        wait_timeout: 1200
      # when: create_assisted_installer and create_vms.changed

    - name: Obtain OpenShift cluster credentials
      register: credentials
      rhpds.assisted_installer.get_credentials:
        cluster_id: "{{ cluster_id }}"
        offline_token: "{{ rh_console_offline_token }}"
      # when: create_assisted_installer and create_vms.changed

    - name: Display credentials
      ansible.builtin.debug:
        var: credentials.result
      # when: create_assisted_installer and create_vms.changed

  handlers:
    - name: Reload BIND9
      ansible.builtin.service:
        name: bind9
        state: reloaded
      delegate_to: dns.rh-lab.morey.tech
      run_once: true
